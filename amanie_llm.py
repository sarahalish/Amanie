# -*- coding: utf-8 -*-
"""Amanie_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10CMOXq7L6WwoV3Yyp9PuomtBVB8o7mjW
"""

!pip install langchain_community

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
# !pip install langchain faiss-cpu sentence-transformers
# !pip install transformers accelerate bitsandbytes
#

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",  # Instruct fine-tuned model
    max_seq_length = 4096, # Increased max_seq_length to accommodate longer inputs
    dtype = None,
    load_in_4bit = True,
)

# تجهيز النموذج للتوليد
FastLanguageModel.for_inference(model)

from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

# إعداد الـ pipeline مع النموذج
qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=False,  # استخدام Greedy decoding
    return_full_text=False # إضافة هذا لضمان إرجاع النص الجديد فقط
)

# لفه بـ HuggingFacePipeline
hf_llm = HuggingFacePipeline(pipeline=qa_pipeline)

from langchain_community.embeddings import HuggingFaceEmbeddings

# تحميل نموذج embeddings متعدد اللغات
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

import json
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
import os

# ⚠️ ملاحظة هامة: تأكد أنك تعمل ضمن بيئة Python تم تثبيت فيها المكتبات التالية:
# pip install langchain-community faiss-cpu sentence-transformers

# ✅ 1. تحديد مسار ملف JSON
# سنفترض أن ملف البيانات الذي تم إنشاؤه مسبقًا متاح هنا.
json_file_path = '/content/AMANIE.txt'

# --- نموذج التضمين المخصص للغة العربية ---
# أفضل نموذج تضمين متعدد اللغات لدعم اللغة العربية في RAG
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
print(f"✅ سيتم استخدام نموذج التضمين: {EMBEDDING_MODEL_NAME}")

# ✅ 2. تحميل البيانات من ملف JSON
try:
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"✅ تم تحميل {len(data)} سجل من ملف البيانات بنجاح.")
except FileNotFoundError:
    print(f"❌ خطأ: لم يتم العثور على الملف {json_file_path}. يرجى التأكد من وجوده في نفس المسار.")
    exit()
except Exception as e:
    print(f"❌ خطأ أثناء قراءة ملف JSON: {e}")
    exit()

# ✅ 3. تحويل البيانات (القواميس) إلى مستندات LangChain
# الملف يحتوي على قائمة من القواميس، حيث كل قاموس يمثل مستندًا
documents = []
for item in data:
    # item['page_content'] يحتوي على السؤال والجواب بالعربي والإنجليزي
    # item['metadata'] يحتوي على التصنيفات (Category, Topic, Source)
    doc = Document(
        page_content=item.get('page_content', ''),
        metadata=item.get('metadata', {})
    )
    documents.append(doc)

# ✅ 4. تقسيم المستندات إلى مقاطع صغيرة (Chunks)
# تقسيم النصوص إلى مقاطع صغيرة ضروري لعملية الاسترجاع في RAG
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,        # حجم المقطع (Chunk)
    chunk_overlap=50,      # التداخل بين المقاطع لضمان السياق
    separators=["\n\n", "\n", ".", " "] # فواصل مفيدة للغة العربية
)
docs = text_splitter.split_documents(documents)
print(f"✅ تم تقسيم المستندات إلى {len(docs)} مقطع (Chunk) جاهز للتضمين.")

# ✅ 5. تحميل نموذج التضمين (Embedding Model)
try:
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cpu'} # يمكن تغيير 'cpu' إلى 'cuda' إذا كان لديك GPU
    )
    print("✅ تم تحميل نموذج التضمين بنجاح.")
except Exception as e:
    print(f"❌ خطأ في تحميل نموذج HuggingFaceEmbeddings: {e}")
    print("يرجى التأكد من تثبيت 'sentence-transformers'.")
    exit()


# ✅ 6. إنشاء قاعدة بيانات FAISS من المستندات المقسمة
# FAISS هي مكتبة فعالة للبحث السريع عن المتجهات
db = FAISS.from_documents(docs, embedding_model)
print("✅ تم بناء قاعدة بيانات FAISS بنجاح!")

# ✅ 7. حفظ قاعدة البيانات لاستخدامها لاحقاً
db_path = "absher_faiss_db"
db.save_local(db_path)
print(f"✅ تم حفظ قاعدة بيانات المتجهات محلياً في المسار: {db_path}")

# --- مثال على كيفية استخدام قاعدة البيانات التي تم إنشاؤها ---
# الآن، يمكنك استخدام قاعدة البيانات للبحث عن إجابات
query = "كيف يمكنني الحصول على الدعم السكني من سكني؟"
print(f"\n--- اختبار الاسترجاع (Retrieval Test) للاستعلام: '{query}' ---")

# استرجاع أفضل 3 مقاطع (Top 3 relevant chunks)
results = db.similarity_search(query, k=3)

for i, doc in enumerate(results):
    print(f"\n[النتيجة رقم {i+1}] (المصدر: {doc.metadata.get('source', 'N/A')} - الفئة: {doc.metadata.get('topic', 'N/A')}):")
    # طباعة محتوى النص المسترجع
    print(doc.page_content)
# ----------------------------------------------------------------------

# الخطوة التالية: استخدام هذه المقاطع المسترجعة مع نموذج LLM (مثل Jais أو Mistral) لتوليد الإجابة النهائية.

retriever = db.as_retriever(
    search_type="mmr",    # Maximal Marginal Relevance
    search_kwargs={"k": 3}  # استرجاع أفضل 3 مستندات
)

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.run_be_passthrough import RunnablePassthrough
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
import textwrap

# --- 1. تحديث قالب الموجه (System Prompt) لمشروع أماني ---
def get_amani_system_prompt():
    """
    يعيد موجه النظام الصارم لمشروع أماني لضمان الموثوقية التامة ومنع التخريف.
    """
    prompt = """
    أنت مساعد ذكي متخصص في خدمة ضيوف الرحمن ضمن مشروع 'أماني'. مهمتك الأساسية هي تقديم معلومات رسمية وموثوقة فقط بناءً على المراجع المقدمة لك (Context).

    التعليمات الصارمة:
    1. الالتزام بالمصدر: أجب فقط باستخدام المعلومات الموجودة في المراجع المرفقة (Context). إذا لم تجد الإجابة، قل بوضوح: 'عذراً، لا تتوفر معلومة رسمية حالياً حول هذا الموضوع'.
    2. محاربة الإشاعات: إذا سألك المستخدم عن إشاعة أو معلومة غير موجودة في مراجعك، قم بنفيها بلباقة وأرشده للمعلومة الرسمية البديلة المتوفرة في السياق.
    3. دعم اللغات: أجب بنفس اللغة التي يسأل بها المستخدم (عربي أو إنجليزي).
    4. منع التخريف: لا تستخدم معلوماتك العامة السابقة إذا كانت تخالف أو لا توجد في المراجع المرفقة.
    5. الدقة والاختصار: اجعل إجاباتك قصيرة، مباشرة، ومهذبة جداً.
    6. تحديثات اللحظة: إذا كانت المعلومة تتعلق بالزحام أو الطقس، أكد أنها 'بناءً على التحديث اللحظي'.

    السياق المسترجع (Context):
    {context}
    """
    return textwrap.dedent(prompt).strip()

# استخراج التعليمات الجديدة
system_instruction = get_amani_system_prompt()

# --- 2. إعداد القالب (Prompt Template) ---
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_instruction),
        ("human", "{question}"),
    ]
)

# --- 3. دالة تنسيق المستندات (Format Docs) ---
def format_docs(docs):
    if not docs:
        return "لا توجد مراجع رسمية متوفرة لهذا السؤال."
    # دمج محتوى المستندات المسترجعة بوضوح
    return "\n\n".join(doc.page_content for doc in docs)

# --- 4. بناء السلسلة (RAG Chain) المحدثة ---

# دالة مساعدة لاستخراج النص من الموديل (تجنب أخطاء النوع)
def extract_text_content(output):
    if hasattr(output, 'content'):
        return output.content
    if isinstance(output, list) and len(output) > 0:
        return str(output[0])
    return str(output)

rag_chain = (
    RunnablePassthrough.assign(
        context=itemgetter("query") | retriever | format_docs
    )
    | {
        "context": itemgetter("context"),
        "question": itemgetter("query")
    }
    | prompt
    | hf_llm  # نموذج اللغة الخاص بك
    | extract_text_content
    | StrOutputParser()
)

# --- مثال على الاستخدام ---
# response = rag_chain.invoke({"query": "ما هي حالة الزحام الآن في صحن الطواف؟"})
# print(response)